#zeppelin notebook - integraci√≥n completa 

%spark
//IMPORTAMOS SPARK Y FUNCTIOS
spark
import org.apache.spark.sql.functions._

%spark
//CREAMOS LA TABLA POR PRIMERA VEZ EN HIVE
val a = spark.read.parquet("gs://daas-metrics-analytics-warehouse/datasets/companies").select(col("*"), lit("current").as("companie_state")).write.saveAsTable("default.companies2")

%spark
//APPEND PARA ACTUALIZAR LA TABLA
spark.read.parquet("gs://daas-metrics-analytics-warehouse/datasets/companies_update").select(col("*"), lit("new").as("companie_state")).createOrReplaceTempView("companies_agg")
val b = spark.sql("select id, entity_id, attribute_set_id, created_at, updated_at, name, address, city, province, zip_code, country_code, industry_id, active, google_ads_id, intent_monthly_investment, latitude, longitude, currency_code, date_timezone, companie_state from companies_agg group by id, entity_id, attribute_set_id, created_at, updated_at, name, address, city, province, zip_code, country_code, industry_id,active, google_ads_id, intent_monthly_investment, latitude, longitude, currency_code, date_timezone, companie_state")
b.write.mode("append").saveAsTable("default.companies2")

%spark
//CREAR TABLA POR PRIMERA VEZ EN POSTGRES
//preparo el dataframe para ingestar en postgres
spark.sql("use default")
val psqlTable = spark.sql("select * from companies2")
//Escribir en postgres
psqlTable.write.format("jdbc").option("url", "jdbc:postgresql://54.38.113.190:5432/daas_ads").option("driver", "org.postgresql.Driver").option("dbtable", "sparktest2").option("user", "dbmaster").option("password", "WrxI6WrPJF").save()

//APPEND PARA ACTUALIZAR TABLA EXISTENTE
//dataframe
spark.sql("use default")
val psqlTable = spark.sql("select * from companies2")
//create properties object
val prop = new java.util.Properties
prop.setProperty("driver", "org.postgresql.Driver")
prop.setProperty("user", "dbmaster")
prop.setProperty("password", "WrxI6WrPJF") 
//jdbc mysql url - destination database is named "data"
val url = "jdbc:postgresql://54.38.113.190:5432/daas_ads"
//destination database table 
val table = "sparktest2"
//write data from spark dataframe to database and exist table
psqlTable.write.mode("append").jdbc(url, table, prop)
